{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Assistants API Enterprise Application Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "OpenAI's Assistants API, launched in late 2023, offers a powerful new option for enterprise AI application development. Compared to the traditional Chat Completions API, the Assistants API provides more comprehensive conversation management, file handling, and tool calling capabilities, making it particularly suitable for building complex enterprise applications.\n",
    "\n",
    "## Core Advantages\n",
    "\n",
    "- Built-in conversation thread management\n",
    "- Native file processing capabilities\n",
    "- Unified tool calling interface\n",
    "- Better context management\n",
    "- Simplified state tracking\n",
    "\n",
    "## Core Feature Analysis\n",
    "### Assistant Creation and Management\n",
    "\n",
    "Assistant is the core component of the system, representing an AI assistant with specific capabilities and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def create_enterprise_assistant():\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Data Analysis Assistant\",\n",
    "        instructions=\"\"\"You are a professional data analysis assistant responsible for:\n",
    "        1. Analyzing user-uploaded data files\n",
    "        2. Generating data visualizations\n",
    "        3. Providing data insights\n",
    "        Please communicate in professional yet accessible language.\"\"\",\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        tools=[\n",
    "            {\"type\": \"code_interpreter\"},\n",
    "            {\"type\": \"retrieval\"}\n",
    "        ]\n",
    "    )\n",
    "    return assistant\n",
    "\n",
    "# Update Assistant Configuration\n",
    "def update_assistant(assistant_id):\n",
    "    updated = client.beta.assistants.update(\n",
    "        assistant_id=assistant_id,\n",
    "        name=\"Enhanced Data Analysis Assistant\",\n",
    "        instructions=\"Updated instructions...\",\n",
    "    )\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread Management\n",
    "\n",
    "Thread is the core mechanism for managing conversation context, with each thread representing a complete conversation session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def manage_conversation():\n",
    "    # Create new thread\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    # Add user message\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"Please analyze the trends in this sales data\"\n",
    "    )\n",
    "\n",
    "    # Run assistant\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=\"asst_xxx\"\n",
    "    )\n",
    "\n",
    "    # Get run results\n",
    "    while True:\n",
    "        run_status = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run_status.status == 'completed':\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Get assistant reply\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id\n",
    "    )\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Handling Best Practices\n",
    "\n",
    "The Assistants API supports processing various file formats, including PDF, Word, Excel, CSV, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_files():\n",
    "    # Upload file\n",
    "    file = client.files.create(\n",
    "        file=open(\"sales_data.csv\", \"rb\"),\n",
    "        purpose='assistants'\n",
    "    )\n",
    "\n",
    "    # Attach file to message\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=\"thread_xxx\",\n",
    "        role=\"user\",\n",
    "        content=\"Please analyze this sales data\",\n",
    "        file_ids=[file.id]\n",
    "    )\n",
    "\n",
    "    # File processing error handling\n",
    "    try:\n",
    "        # File processing logic\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        logging.error(f\"File processing error: {str(e)}\")\n",
    "        # Implement retry logic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enterprise Optimization Strategies\n",
    "\n",
    "### 1. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssistantManager:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "        self.cache = {}  # Simple memory cache\n",
    "\n",
    "    def get_assistant(self, assistant_id):\n",
    "        # Implement caching mechanism\n",
    "        if assistant_id in self.cache:\n",
    "            return self.cache[assistant_id]\n",
    "\n",
    "        assistant = self.client.beta.assistants.retrieve(assistant_id)\n",
    "        self.cache[assistant_id] = assistant\n",
    "        return assistant\n",
    "\n",
    "    def create_thread_with_retry(self, max_retries=3):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return self.client.beta.threads.create()\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Optimization\n",
    "\n",
    "Token usage optimization is key to controlling costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_prompt(prompt: str) -> str:\n",
    "    \"\"\"Optimize prompt to reduce token usage\"\"\"\n",
    "    # Remove excess whitespace\n",
    "    prompt = \" \".join(prompt.split())\n",
    "    # Compress repetitive instructions\n",
    "    prompt = prompt.replace(\"please note\", \"\")\n",
    "    return prompt\n",
    "\n",
    "def calculate_cost(messages: list) -> float:\n",
    "    \"\"\"Estimate API call costs\"\"\"\n",
    "    token_count = 0\n",
    "    for msg in messages:\n",
    "        token_count += len(msg['content']) / 4  # Rough estimate\n",
    "\n",
    "    # GPT-4 pricing (example)\n",
    "    input_cost = token_count * 0.00003\n",
    "    output_cost = token_count * 0.00006\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error Handling\n",
    "\n",
    "Enterprise applications require comprehensive error handling:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "class AssistantError(Exception):\n",
    "    \"\"\"Custom assistant error\"\"\"\n",
    "    pass\n",
    "\n",
    "def handle_assistant_call(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except openai.APIError as e:\n",
    "            logging.error(f\"API error: {str(e)}\")\n",
    "            raise AssistantError(\"API call failed\")\n",
    "        except openai.APIConnectionError:\n",
    "            logging.error(\"Connection error\")\n",
    "            raise AssistantError(\"Network connection failed\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unknown error: {str(e)}\")\n",
    "            raise\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Environment Best Practices\n",
    "\n",
    "### 1. Monitoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicated timeseries in CollectorRegistry: {'assistant_api_calls', 'assistant_api_calls_total', 'assistant_api_calls_created'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprometheus_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter, Histogram\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define monitoring metrics\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m api_calls \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant_api_calls_total\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal API calls\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m response_time \u001b[38;5;241m=\u001b[39m Histogram(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant_response_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse time in seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmonitor_api_call\u001b[39m(func):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/prometheus_client/metrics.py:143\u001b[0m, in \u001b[0;36mMetricWrapperBase.__init__\u001b[0;34m(self, name, documentation, labelnames, namespace, subsystem, unit, registry, _labelvalues)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labelvalues:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Register the multi-wrapper parent metric, or if a label-less metric, the whole shebang.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m registry:\n\u001b[0;32m--> 143\u001b[0m         registry\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/prometheus_client/registry.py:43\u001b[0m, in \u001b[0;36mCollectorRegistry.register\u001b[0;34m(self, collector)\u001b[0m\n\u001b[1;32m     41\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names_to_collectors)\u001b[38;5;241m.\u001b[39mintersection(names)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicates:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicated timeseries in CollectorRegistry: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     45\u001b[0m             duplicates))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names_to_collectors[name] \u001b[38;5;241m=\u001b[39m collector\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicated timeseries in CollectorRegistry: {'assistant_api_calls', 'assistant_api_calls_total', 'assistant_api_calls_created'}"
     ]
    }
   ],
   "source": [
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Define monitoring metrics\n",
    "api_calls = Counter('assistant_api_calls_total', 'Total API calls')\n",
    "response_time = Histogram('assistant_response_seconds', 'Response time in seconds')\n",
    "\n",
    "def monitor_api_call(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        api_calls.inc()\n",
    "        with response_time.time():\n",
    "            return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: What's the weather today?\n",
      "API calls: [Metric(assistant_api_calls, Total API calls, counter, , [Sample(name='assistant_api_calls_total', labels={}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_api_calls_created', labels={}, value=1746714861.4454231, timestamp=None, exemplar=None)])]\n",
      "Response time histogram: [Metric(assistant_response_seconds, Response time in seconds, histogram, , [Sample(name='assistant_response_seconds_bucket', labels={'le': '0.005'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.01'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.025'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.05'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.075'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.1'}, value=0.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.25'}, value=6.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.5'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '0.75'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '1.0'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '2.5'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '5.0'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '7.5'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '10.0'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_bucket', labels={'le': '+Inf'}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_count', labels={}, value=16.0, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_sum', labels={}, value=4.778872455994133, timestamp=None, exemplar=None), Sample(name='assistant_response_seconds_created', labels={}, value=1746714861.445499, timestamp=None, exemplar=None)])]\n"
     ]
    }
   ],
   "source": [
    "# 假设这是你的API调用函数\n",
    "@monitor_api_call\n",
    "def call_openai_assistant(prompt: str):\n",
    "    # 模拟调用OpenAI Assistant API\n",
    "    import time\n",
    "    import random\n",
    "    \n",
    "    # 模拟API处理时间\n",
    "    time.sleep(random.uniform(0.1, 0.5))\n",
    "    \n",
    "    # 返回模拟结果\n",
    "    return f\"Processed: {prompt}\"\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 调用被监控的函数\n",
    "    result = call_openai_assistant(\"What's the weather today?\")\n",
    "    print(result)\n",
    "    \n",
    "    # 多次调用以观察监控效果\n",
    "    for i in range(3):\n",
    "        call_openai_assistant(f\"Query {i}\")\n",
    "\n",
    "    print(\"API calls:\", api_calls.collect())\n",
    "    print(\"Response time histogram:\", response_time.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logging Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import structlog\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "def setup_logging():\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            structlog.stdlib.add_log_level,\n",
    "            structlog.processors.JSONRenderer()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def log_assistant_activity(thread_id, action, status):\n",
    "    logger.info(\"assistant_activity\",\n",
    "                thread_id=thread_id,\n",
    "                action=action,\n",
    "                status=status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Case: Intelligent Customer Service System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerServiceAssistant:\n",
    "    def __init__(self):\n",
    "        self.assistant = create_enterprise_assistant()\n",
    "        self.thread_manager = ThreadManager()\n",
    "\n",
    "    def handle_customer_query(self, customer_id: str, query: str):\n",
    "        # Get or create customer thread\n",
    "        thread = self.thread_manager.get_customer_thread(customer_id)\n",
    "\n",
    "        # Add query to thread\n",
    "        message = client.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=query\n",
    "        )\n",
    "\n",
    "        # Run assistant and get response\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=self.assistant.id\n",
    "        )\n",
    "\n",
    "        # Wait for and return results\n",
    "        response = self.wait_for_response(thread.id, run.id)\n",
    "        return response\n",
    "\n",
    "    @monitor_api_call\n",
    "    def wait_for_response(self, thread_id, run_id):\n",
    "        while True:\n",
    "            run_status = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread_id,\n",
    "                run_id=run_id\n",
    "            )\n",
    "            if run_status.status == 'completed':\n",
    "                messages = client.beta.threads.messages.list(\n",
    "                    thread_id=thread_id\n",
    "                )\n",
    "                return messages.data[0].content\n",
    "            elif run_status.status == 'failed':\n",
    "                raise AssistantError(\"Processing failed\")\n",
    "            time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The Assistants API provides powerful and flexible functionality for enterprise applications, but effective use in production environments requires attention to:\n",
    "\n",
    "- Proper thread management strategy\n",
    "- Comprehensive error handling\n",
    "- Reasonable cost control measures\n",
    "- Reliable monitoring and logging systems\n",
    "- Optimized performance and scalability\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Establish complete test suite\n",
    "- Implement granular cost monitoring\n",
    "- Optimize response times\n",
    "- Establish backup and failover mechanisms\n",
    "- Enhance security controls\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
